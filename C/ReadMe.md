#### RNN regression
* 1 hidden layer
    * Adjusable hidden cell count
* tanh() hidden node
* Unbounded output node

#### TODO
* ~~Dynamic learning rate~~
* ~~Gradient check~~
* Clean up the code
* Abstract matrix operation
* Abstract squash function derivative for gradeint descent calculation
* Model parameter import/export
* LSTM!

#### Reference
* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/
